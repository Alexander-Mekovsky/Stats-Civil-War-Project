civil.war ~ .,
data = df.train,
kernel = "linear",
ranges = list(cost = 10^seq(-2, 0, by = 0.1)),
probability = TRUE)
best.model <- tune.out$best.model
resp.pred <- predict(best.model, newdata = df.test, probability = TRUE)
prob.attr <- attr(resp.pred, "probabilities")
resp.prob <- prob.attr[, "YES"]
# Compute AUC
roc_svm <- roc(df.test$civil.war, resp.prob)
auc_svm <- auc(roc_svm)
print(paste("SVM (Linear Kernel) AUC:", round(auc_svm, 3)))
# Youden's J for optimal threshold
optimal_threshold <- coords(roc_svm, "best", ret = "threshold", best.method = "youden")
print(paste("SVM Optimal Threshold:", round(optimal_threshold, 3)))
# MCR and confusion matrix
final_predictions <- ifelse(resp.prob >= as.numeric(optimal_threshold), "YES", "NO")
cat("SVM MCR:", mean(final_predictions != df.test$civil.war), "\n")
conf_matrix <- table(Predicted = final_predictions, Actual = df.test$civil.war)
print(conf_matrix)
# Chunk 19
# Load the class library for KNN
library(class)
# Scale the predictors; KNN works better when features are on the same scale
train_scaled <- scale(train[,-which(names(train) %in% c("civil.war", "dominance"))])
test_scaled <- scale(test[,-which(names(test) %in% c("civil.war", "dominance"))])
# Ensure the target variable is a factor
train_labels <- as.factor(train$civil.war)
test_labels <- as.factor(test$civil.war)
# Choose a value for k; often chosen via cross-validation
k_value <- 5
# Fit KNN model
knn_pred <- knn(train = train_scaled, test = test_scaled, cl = train_labels, k = k_value, prob = TRUE)
# Extract class probabilities
knn_probs <- attr(knn_pred, "prob")
knn_probs <- ifelse(knn_pred == "YES", knn_probs, 1 - knn_probs)
# Compute ROC and AUC
library(pROC)
roc_knn <- roc(test_labels, knn_probs)
auc_knn <- auc(roc_knn)
# Chunk 20
# Create a data frame to store AUC scores
model_aucs <- data.frame(
Model = c("Logistic Regression", "Random Forest", "SVM", "Decision Trees", "KNN"),
AUC = c(auc_log, auc_rf, auc_svm, auc_dt, auc_knn)
)
# Print the table
print(model_aucs)
# Chunk 21
library(ggplot2)
library(pROC)
# Create data frames for each ROC curve with specificity, sensitivity, and model label
roc_data_log <- data.frame(Specificity=1 - roc_log$specificities, Sensitivity=roc_log$sensitivities, Model="Logistic Regression")
roc_data_rf <- data.frame(Specificity=1 - roc_rf$specificities, Sensitivity=roc_rf$sensitivities, Model="Random Forest")
roc_data_svm <- data.frame(Specificity=1 - roc_svm$specificities, Sensitivity=roc_svm$sensitivities, Model="SVM")
roc_data_dt <- data.frame(Specificity=1 - roc_dt$specificities, Sensitivity=roc_dt$sensitivities, Model="Decision Trees")
roc_data_knn <- data.frame(Specificity=1 - roc_knn$specificities, Sensitivity=roc_knn$sensitivities, Model="KNN")
# Combine all data frames into one
roc_data <- rbind(roc_data_log, roc_data_rf, roc_data_svm, roc_data_dt, roc_data_knn)
roc_plot <- ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Model)) +
geom_line() +
labs(title = "ROC Curve Comparison",
x = "1 - Specificity (False Positive Rate)",
y = "Sensitivity (True Positive Rate)") +
scale_color_brewer(palette = "Set1", name = "Model") +
theme_minimal() +
theme(legend.position = "bottom")
print(roc_plot)
# Chunk 1
suppressMessages(library(dplyr))
suppressMessages(library(tidyverse))
library(dplyr)
library(tidyr)
library(ggplot2)
df <- read_csv("civilWar.csv")
# summarize the data to check for ouliers
summary(df)
#check fi there is nas in the dataset
sum(is.na(df))
#The data do not hace any outlier
head(df)
# Chunk 2: creating_histogram
df_cleaned <- df %>%
select(-c(dominance, civil.war)) %>% # removing th ecategorical data
pivot_longer(
cols = everything(),
names_to = "variable",
values_to = "value"
)
# Assuming df is your original dataframe
df_cleaned <- df %>%
select(-c(dominance, civil.war)) %>%
pivot_longer(
cols = everything(),
names_to = "variable",
values_to = "value"
)
# Creating histograms
plot <- ggplot(df_cleaned, aes(x = value,fill = variable)) +
geom_histogram(bins = 10) +
facet_wrap(~ variable, scales = "free_x") +
theme_minimal() +
theme(
panel.grid.major = element_blank(), # remove major grid lines
panel.grid.minor = element_blank()  # remove minor grid lines
) +
labs(x = NULL, y = "Frequency") +
theme(legend.title = element_blank(),legend.position = "top",legend.direction = "horizontal")
print(plot)
# Chunk 3
df$schooling <- sqrt(df$schooling)		 # transforming the scholing predictor
df_cleaned <- df %>%
select(-c(dominance, civil.war)) %>%
pivot_longer(
cols = everything(),
names_to = "variable",
values_to = "value"
)
# Creating histograms
plot <- ggplot(df_cleaned, aes(x = value,fill = variable)) +
geom_histogram(bins = 10) +
facet_wrap(~ variable, scales = "free_x") +
theme_minimal() +
theme(
panel.grid.major = element_blank(), # remove major grid lines
panel.grid.minor = element_blank()  # remove minor grid lines
) +
labs(x = NULL, y = "Frequency") +
theme(legend.title = element_blank(),legend.position = "top",legend.direction = "horizontal")
print(plot)
# Chunk 4: target-distribution
df_targeted <- df %>%
select(civil.war, dominance) %>%
pivot_longer(
names_to = "variable",
values_to = "value",
cols = everything()
)
ggplot(df_targeted) +
geom_bar(aes(x = variable, fill = value)) +
scale_fill_manual(values = c("NO" = "green", "YES" = "red")) +
ggtitle("Distribution of Civil War Occurrence and Dominance") +
xlab("Variables") +
ylab("Count") +
theme(
panel.grid.major = element_blank(), # remove major grid lines
panel.grid.minor = element_blank(), # remove minor grid lines
panel.background = element_blank(), # remove background
axis.line = element_line(color = "black"), # add axis lines
legend.position = c(0.6, 0.5),
legend.title = element_blank() # remove legend title
)
# Chunk 5: pairplot
pairs(df[, c("exports", "schooling", "growth", "concentration", "lnpop", "fractionalization")],
main = "Pairplot of Numerical Features",
col = ifelse(df$civil.war == "YES", "red", "blue"))
# Chunk 6: correlation-heatmap
library(corrplot)
num_features <- df[, c('exports', 'schooling', 'growth', 'concentration', 'lnpop', 'fractionalization')]
cor_matrix <- cor(num_features, use = "complete.obs")
corrplot(cor_matrix, method = "circle", type = "lower", tl.col = "black", tl.srt = 45)
# Chunk 7: split-dataset
df$civil.war <- factor(df$civil.war, levels = c("NO", "YES"))
df$dominance <- factor(df$dominance, levels = c("NO", "YES"))
set.seed(123)
train_idx <- sample(nrow(df),round(0.8*nrow(df)))
train <- df[train_idx, ]
test <- df[-train_idx, ]
# Chunk 8: logistic-regression
suppressMessages(library(pROC))
log_mdl <- glm(civil.war ~ ., data = train, family = binomial)
log_probs <- predict(log_mdl, newdata = test, type = "response")
roc_log <- roc(test$civil.war, log_probs)
auc_log <- auc(roc_log)
threshold_log <- as.numeric(coords(roc_log, "best", ret = "threshold", best.method = "youden")[1])
log_pred_class <- ifelse(log_probs > threshold_log, "YES", "NO")
# Missclassification rate
mcr_log <- mean(log_pred_class != test$civil.war)
# Chunk 9: summary logistic regression
summary(log_mdl)
# Chunk 10: results
cat("AUC (logistic regression):", round(auc_log, 3), "\n")
cat("MCR (logistic regression):", round(mcr_log, 3), "\n")
# Chunk 11: confusion-matrix
log_conf_matrix <- table(Predicted = log_pred_class, Actual = test$civil.war)
print(log_conf_matrix)
# Chunk 12: metrics
conf_matrix <- log_conf_matrix
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- conf_matrix["YES", "YES"] / sum(conf_matrix["YES", ])
recall <- conf_matrix["YES", "YES"] / sum(conf_matrix[, "YES"])
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("Logistic regression:","\n","Accuracy:", round(accuracy, 3), "\n")
cat("Precision:", round(precision, 3), "\n")
cat("Recall:", round(recall, 3), "\n")
cat("F1 Score:", round(f1_score, 3), "\n")
# Chunk 13
df.train <- train
# Standardize numerical variables
df.train$exports <- scale(df.train$exports)
df.train$schooling <- scale(df.train$schooling)
df.train$growth <- scale(df.train$growth)
df.train$concentration <- scale(df.train$concentration)
df.train$lnpop <- scale(df.train$lnpop)
df.train$fractionalization <- scale(df.train$fractionalization)
# Fit a new logistic regression model
model_scaled <- glm(civil.war ~ ., data = df.train, family = binomial)
# Summary of the scaled model
summary(model_scaled)
# Chunk 14
set.seed(408)
suppressMessages(library(randomForest))
suppressMessages(library(pROC))
df.train <- train
# AUC value
rf.out = randomForest(civil.war ~ ., data=df.train,importance=TRUE)
out.pred = predict(rf.out,newdata=test,type="prob")[,2]
roc_rf <- roc(test$civil.war, out.pred)
auc_rf <- auc(roc_rf)
print(paste("Random Forest AUC:", round(auc_rf, 3)))
# Youden’s J
optimal_threshold <- coords(roc_rf, "best", ret="threshold", best.method="youden")
print(paste("Random Forest Optimal Threshold:", round(optimal_threshold, 3)))
# MCR and confusion matrix
resp.pred <- ifelse(out.pred >= as.numeric(optimal_threshold), "YES","NO")
resp.test = test$civil.war
cat('Random Forest MCR: ',mean(resp.pred!=resp.test),'\n')
tab <- table(resp.pred,resp.test)
print(tab)
# Chunk 15
varImpPlot(rf.out,type=1, main = "Variable Importance")
# Chunk 16
library(rpart)
df.train <- train
df.test <- test
# AUC value
rpart.out <- rpart(civil.war~.,data=df.train)
resp.pred <- predict(rpart.out,newdata=df.test,type="prob")[,2]
roc_dt <- roc(df.test$civil.war, resp.pred)
auc_dt <- auc(roc_dt)
print(paste("Tree AUC:", round(auc_dt, 3)))
# Youden’s J
optimal_threshold <- coords(roc_dt, "best", ret="threshold", best.method="youden")
print(paste("Tree Optimal Threshold:", round(optimal_threshold, 3)))
# MCR and confusion matrix
resp.pred <- ifelse(resp.pred >= as.numeric(optimal_threshold), "YES","NO")
resp.test = df.test$civil.war
cat('Tree MCR: ',mean(resp.pred!=resp.test),'\n')
tab <- table(resp.pred,resp.test)
print(tab)
# Chunk 17
# visualization
library(rpart.plot)
rpart.pruned <- prune(rpart.out,cp=0.03)
rpart.plot(rpart.pruned,extra=104)
# Chunk 18
set.seed(408)
suppressMessages(library(e1071))
df.train <- train
df.test <- test
tune.out <- tune(svm,
civil.war ~ .,
data = df.train,
kernel = "linear",
ranges = list(cost = 10^seq(-2, 0, by = 0.1)),
probability = TRUE)
best.model <- tune.out$best.model
resp.pred <- predict(best.model, newdata = df.test, probability = TRUE)
prob.attr <- attr(resp.pred, "probabilities")
resp.prob <- prob.attr[, "YES"]
# Compute AUC
roc_svm <- roc(df.test$civil.war, resp.prob)
auc_svm <- auc(roc_svm)
print(paste("SVM (Linear Kernel) AUC:", round(auc_svm, 3)))
# Youden's J for optimal threshold
optimal_threshold <- coords(roc_svm, "best", ret = "threshold", best.method = "youden")
print(paste("SVM Optimal Threshold:", round(optimal_threshold, 3)))
# MCR and confusion matrix
final_predictions <- ifelse(resp.prob >= as.numeric(optimal_threshold), "YES", "NO")
cat("SVM MCR:", mean(final_predictions != df.test$civil.war), "\n")
conf_matrix <- table(Predicted = final_predictions, Actual = df.test$civil.war)
print(conf_matrix)
# Chunk 19
# Load the class library for KNN
library(class)
# Scale the predictors; KNN works better when features are on the same scale
train_scaled <- scale(train[,-which(names(train) %in% c("civil.war", "dominance"))])
test_scaled <- scale(test[,-which(names(test) %in% c("civil.war", "dominance"))])
# Ensure the target variable is a factor
train_labels <- as.factor(train$civil.war)
test_labels <- as.factor(test$civil.war)
# Choose a value for k; often chosen via cross-validation
k_value <- 5
# Fit KNN model
knn_pred <- knn(train = train_scaled, test = test_scaled, cl = train_labels, k = k_value, prob = TRUE)
# Extract class probabilities
knn_probs <- attr(knn_pred, "prob")
knn_probs <- ifelse(knn_pred == "YES", knn_probs, 1 - knn_probs)
# Compute ROC and AUC
library(pROC)
roc_knn <- roc(test_labels, knn_probs)
auc_knn <- auc(roc_knn)
# Chunk 20
# Create a data frame to store AUC scores
model_aucs <- data.frame(
Model = c("Logistic Regression", "Random Forest", "SVM", "Decision Trees", "KNN"),
AUC = c(auc_log, auc_rf, auc_svm, auc_dt, auc_knn)
)
# Print the table
print(model_aucs)
# Chunk 21
library(ggplot2)
library(pROC)
# Create data frames for each ROC curve with specificity, sensitivity, and model label
roc_data_log <- data.frame(Specificity=1 - roc_log$specificities, Sensitivity=roc_log$sensitivities, Model="Logistic Regression")
roc_data_rf <- data.frame(Specificity=1 - roc_rf$specificities, Sensitivity=roc_rf$sensitivities, Model="Random Forest")
roc_data_svm <- data.frame(Specificity=1 - roc_svm$specificities, Sensitivity=roc_svm$sensitivities, Model="SVM")
roc_data_dt <- data.frame(Specificity=1 - roc_dt$specificities, Sensitivity=roc_dt$sensitivities, Model="Decision Trees")
roc_data_knn <- data.frame(Specificity=1 - roc_knn$specificities, Sensitivity=roc_knn$sensitivities, Model="KNN")
# Combine all data frames into one
roc_data <- rbind(roc_data_log, roc_data_rf, roc_data_svm, roc_data_dt, roc_data_knn)
roc_plot <- ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Model)) +
geom_line() +
labs(title = "ROC Curve Comparison",
x = "1 - Specificity (False Positive Rate)",
y = "Sensitivity (True Positive Rate)") +
scale_color_brewer(palette = "Set1", name = "Model") +
theme_minimal() +
theme(legend.position = "bottom")
print(roc_plot)
library(ggplot2)
library(pROC)
# Create data frames for each ROC curve with specificity, sensitivity, and model label
roc_data_log <- data.frame(Specificity=1 - roc_log$specificities, Sensitivity=roc_log$sensitivities, Model="Logistic Regression")
roc_data_rf <- data.frame(Specificity=1 - roc_rf$specificities, Sensitivity=roc_rf$sensitivities, Model="Random Forest")
roc_data_svm <- data.frame(Specificity=1 - roc_svm$specificities, Sensitivity=roc_svm$sensitivities, Model="SVM")
roc_data_dt <- data.frame(Specificity=1 - roc_dt$specificities, Sensitivity=roc_dt$sensitivities, Model="Decision Trees")
roc_data_knn <- data.frame(Specificity=1 - roc_knn$specificities, Sensitivity=roc_knn$sensitivities, Model="KNN")
# Combine all data frames into one
roc_data <- rbind(roc_data_log, roc_data_rf, roc_data_svm, roc_data_dt, roc_data_knn)
roc_plot <- ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Model)) +
geom_line() +
labs(title = "ROC Curve Comparison",
x = "1 - Specificity (False Positive Rate)",
y = "Sensitivity (True Positive Rate)") +
scale_color_brewer(palette = "Set1", name = "Model") +
theme_minimal() +
theme(legend.position = "bottom") +
theme_bw(base_size = 16)
print(roc_plot)
library(ggplot2)
library(pROC)
# Create data frames for each ROC curve with specificity, sensitivity, and model label
roc_data_log <- data.frame(Specificity=1 - roc_log$specificities, Sensitivity=roc_log$sensitivities, Model="Logistic Regression")
roc_data_rf <- data.frame(Specificity=1 - roc_rf$specificities, Sensitivity=roc_rf$sensitivities, Model="Random Forest")
roc_data_svm <- data.frame(Specificity=1 - roc_svm$specificities, Sensitivity=roc_svm$sensitivities, Model="SVM")
roc_data_dt <- data.frame(Specificity=1 - roc_dt$specificities, Sensitivity=roc_dt$sensitivities, Model="Decision Trees")
roc_data_knn <- data.frame(Specificity=1 - roc_knn$specificities, Sensitivity=roc_knn$sensitivities, Model="KNN")
# Combine all data frames into one
roc_data <- rbind(roc_data_log, roc_data_rf, roc_data_svm, roc_data_dt, roc_data_knn)
roc_plot <- ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Model)) +
geom_line(size = 2) +
labs(title = "ROC Curve Comparison",
x = "1 - Specificity (False Positive Rate)",
y = "Sensitivity (True Positive Rate)") +
scale_color_brewer(palette = "Set1", name = "Model") +
theme_minimal() +
theme(legend.position = "bottom") +
theme_bw(base_size = 16)
print(roc_plot)
library(ggplot2)
library(pROC)
# Create data frames for each ROC curve with specificity, sensitivity, and model label
roc_data_log <- data.frame(Specificity=1 - roc_log$specificities, Sensitivity=roc_log$sensitivities, Model="Logistic Regression")
roc_data_rf <- data.frame(Specificity=1 - roc_rf$specificities, Sensitivity=roc_rf$sensitivities, Model="Random Forest")
roc_data_svm <- data.frame(Specificity=1 - roc_svm$specificities, Sensitivity=roc_svm$sensitivities, Model="SVM")
roc_data_dt <- data.frame(Specificity=1 - roc_dt$specificities, Sensitivity=roc_dt$sensitivities, Model="Decision Trees")
roc_data_knn <- data.frame(Specificity=1 - roc_knn$specificities, Sensitivity=roc_knn$sensitivities, Model="KNN")
# Combine all data frames into one
roc_data <- rbind(roc_data_log, roc_data_rf, roc_data_svm, roc_data_dt, roc_data_knn)
roc_plot <- ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Model)) +
geom_line(size = 1.5) +
labs(title = "ROC Curve Comparison",
x = "1 - Specificity (False Positive Rate)",
y = "Sensitivity (True Positive Rate)") +
scale_color_brewer(palette = "Set1", name = "Model") +
theme_minimal() +
theme(legend.position = "bottom") +
theme_bw(base_size = 16)
print(roc_plot)
library(ggplot2)
library(pROC)
# Create data frames for each ROC curve with specificity, sensitivity, and model label
roc_data_log <- data.frame(Specificity=1 - roc_log$specificities, Sensitivity=roc_log$sensitivities, Model="Logistic Regression")
roc_data_rf <- data.frame(Specificity=1 - roc_rf$specificities, Sensitivity=roc_rf$sensitivities, Model="Random Forest")
roc_data_svm <- data.frame(Specificity=1 - roc_svm$specificities, Sensitivity=roc_svm$sensitivities, Model="SVM")
roc_data_dt <- data.frame(Specificity=1 - roc_dt$specificities, Sensitivity=roc_dt$sensitivities, Model="Decision Trees")
roc_data_knn <- data.frame(Specificity=1 - roc_knn$specificities, Sensitivity=roc_knn$sensitivities, Model="KNN")
# Combine all data frames into one
roc_data <- rbind(roc_data_log, roc_data_rf, roc_data_svm, roc_data_dt, roc_data_knn)
roc_plot <- ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Model)) +
geom_line(size = 1) +
labs(title = "ROC Curve Comparison",
x = "1 - Specificity (False Positive Rate)",
y = "Sensitivity (True Positive Rate)") +
scale_color_brewer(palette = "Set1", name = "Model") +
theme_minimal() +
theme(legend.position = "bottom") +
theme_bw(base_size = 16)
print(roc_plot)
library(ggplot2)
library(pROC)
# Create data frames for each ROC curve with specificity, sensitivity, and model label
roc_data_log <- data.frame(Specificity=1 - roc_log$specificities, Sensitivity=roc_log$sensitivities, Model="Logistic Regression")
roc_data_rf <- data.frame(Specificity=1 - roc_rf$specificities, Sensitivity=roc_rf$sensitivities, Model="Random Forest")
roc_data_svm <- data.frame(Specificity=1 - roc_svm$specificities, Sensitivity=roc_svm$sensitivities, Model="SVM")
roc_data_dt <- data.frame(Specificity=1 - roc_dt$specificities, Sensitivity=roc_dt$sensitivities, Model="Decision Trees")
roc_data_knn <- data.frame(Specificity=1 - roc_knn$specificities, Sensitivity=roc_knn$sensitivities, Model="KNN")
# Combine all data frames into one
roc_data <- rbind(roc_data_log, roc_data_rf, roc_data_svm, roc_data_dt, roc_data_knn)
roc_plot <- ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Model)) +
geom_line(size = 1) +
labs(title = "ROC Curve Comparison",
x = "1 - Specificity (False Positive Rate)",
y = "Sensitivity (True Positive Rate)") +
scale_color_brewer(palette = "Set1", name = "Model") +
theme_minimal() +
theme(legend.position = "bottom") +
theme_bw(base_size = 20)
print(roc_plot)
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf_model))
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf.out))
importance_data$Variable <- rownames(importance_data)
ggplot(importance_data, aes(x = MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy))) + geom_point(size = 5, color = "blue") + theme_minimal() + labs(title = "Variable Importance", x = "Mean Decrease in Accuracy", y = "Variable")
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf.out))
importance_data$Variable <- rownames(importance_data)
ggplot(importance_data, aes(x = MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy))) + geom_point(size = 5, color = "blue") + theme_minimal() + labs(title = "Variable Importance", x = "Mean Decrease in Accuracy", y = "Variable") + xlim(0, 40)
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf.out))
importance_data$Variable <- rownames(importance_data)
ggplot(importance_data, aes(x = MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy))) + geom_point(size = 5, color = "red") + theme_minimal() + labs(title = "Variable Importance", x = "Mean Decrease in Accuracy", y = "Variable") + xlim(0, 40)
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf.out))
importance_data$Variable <- rownames(importance_data)
ggplot(importance_data, aes(x = MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy))) + geom_point(size = 3, color = "red") + theme_minimal() + labs(title = "Variable Importance", x = "Mean Decrease in Accuracy", y = "Variable") + xlim(0, 40)
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf.out))
importance_data$Variable <- rownames(importance_data)
ggplot(importance_data, aes(x = MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy))) + geom_point(size = 4, color = "red") + theme_minimal() + labs(title = "Variable Importance", x = "Mean Decrease in Accuracy", y = "Variable") + xlim(0, 40)
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf.out))
importance_data$Variable <- rownames(importance_data)
ggplot(importance_data, aes(x = MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy))) + geom_point(size = 4, color = "red") + labs(title = "Variable Importance", x = "Mean Decrease in Accuracy", y = "Variable") + xlim(0, 40)
#varImpPlot(rf.out,type=1, main = "Variable Importance")
importance_data <- as.data.frame(importance(rf.out))
importance_data$Variable <- rownames(importance_data)
ggplot(importance_data, aes(x = MeanDecreaseAccuracy, y = reorder(Variable, MeanDecreaseAccuracy))) + geom_point(size = 4, color = "red") + theme_minimal() + labs(title = "Variable Importance", x = "Mean Decrease in Accuracy", y = "Variable") + xlim(0, 40)
df_cleaned <- df %>%
select(-c(dominance, civil.war)) %>% # removing th ecategorical data
pivot_longer(
cols = everything(),
names_to = "variable",
values_to = "value"
)
# Assuming df is your original dataframe
df_cleaned <- df %>%
select(-c(dominance, civil.war)) %>%
pivot_longer(
cols = everything(),
names_to = "variable",
values_to = "value"
)
# Creating histograms
plot <- ggplot(df_cleaned, aes(x = value,fill = variable)) +
geom_histogram(bins = 10) +
facet_wrap(~ variable, scales = "free_x") +
theme_minimal() +
theme(
panel.grid.major = element_blank(), # remove major grid lines
panel.grid.minor = element_blank()  # remove minor grid lines
) +
labs(x = NULL, y = "Frequency") +
theme(legend.title = element_blank(),legend.position = "top",legend.direction = "horizontal")
print(plot)
# Load the class library for KNN
library(class)
# Scale the predictors; KNN works better when features are on the same scale
train_scaled <- scale(train[,-which(names(train) %in% c("civil.war", "dominance"))])
test_scaled <- scale(test[,-which(names(test) %in% c("civil.war", "dominance"))])
# Ensure the target variable is a factor
train_labels <- as.factor(train$civil.war)
test_labels <- as.factor(test$civil.war)
# Choose a value for k; often chosen via cross-validation
k_value <- 5
# Fit KNN model
knn_pred <- knn(train = train_scaled, test = test_scaled, cl = train_labels, k = k_value, prob = TRUE)
# Extract class probabilities
knn_probs <- attr(knn_pred, "prob")
knn_probs <- ifelse(knn_pred == "YES", knn_probs, 1 - knn_probs)
# Compute ROC and AUC
library(pROC)
roc_knn <- roc(test_labels, knn_probs)
auc_knn <- auc(roc_knn)
# MCR and confusion matrix
final_predictions <- ifelse(knn_probs >= as.numeric(optimal_threshold), "YES", "NO")
cat("KNN MCR:", mean(final_predictions != df.test$civil.war), "\n")
conf_matrix <- table(Predicted = final_predictions, Actual = df.test$civil.war)
print(conf_matrix)
